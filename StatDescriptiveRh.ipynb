{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfo:       num_id,num_im,codebur,lieu_af,motif_affect,fonction_af,division_af,service_af,direction_af,date_ps\n",
      "0     2008DEC5,275 850,34AS,ANTSIRANANA,COMPLEMENT D...                                                \n",
      "1     2009DEC8,318 069,22TP,TOAMASINA-PETROLES,,CHEF...                                                \n",
      "2     2000DEC4,275 855,34AS,ANTSIRANANA,COMPLEMENT D...                                                \n",
      "3     2001DEC2,275 855,12IV,IVATO-AEROPORT,COMPLEMEN...                                                \n",
      "4     2010DEC7,275 855,22TP,TOAMASINA-PETROLES,COMPL...                                                \n",
      "...                                                 ...                                                \n",
      "4448  2019DEC1087,433 883,SAE,SERVICE DES ACTIONS EC...                                                \n",
      "4449  2019DEC1088,347 483,SLF,SERVICE DE LA LUTTE CO...                                                \n",
      "4450  2019DEC1089,353 660,SLF,SERVICE DE LA LUTTE CO...                                                \n",
      "4451  2019DCR14,402 429,SRAR,SERVICE DU RENSEIGNEMEN...                                                \n",
      "4452  2019DEC1079,433 890,SLF,SERVICE DE LA LUTTE CO...                                                \n",
      "\n",
      "[4453 rows x 1 columns]\n",
      "dfo index:  RangeIndex(start=0, stop=4453, step=1)\n",
      "dfo columns:  Index(['num_id,num_im,codebur,lieu_af,motif_affect,fonction_af,division_af,service_af,direction_af,date_ps'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'service_af'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\kitty\\pycharmprojects\\testaffichage\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3360\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kitty\\pycharmprojects\\testaffichage\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kitty\\pycharmprojects\\testaffichage\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'service_af'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23984/2477718533.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[1;31m# Supprimer toutes les observations contenant une valeur manquante (au moins sur une variable)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[0mdfo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"all\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# lorsqu'on veut garder la même table\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m     \u001B[0mdfo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'service_af'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"all\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# lorsqu'on veut garder la même table\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m     \u001B[0mdfo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Annee'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdfo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'date_ps'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0myear\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[0mAnnee\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdfo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Annee'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfillna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kitty\\pycharmprojects\\testaffichage\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3453\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3454\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3455\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3456\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3457\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kitty\\pycharmprojects\\testaffichage\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3361\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3363\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3365\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mis_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasnans\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'service_af'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sqlalchemy\n",
    "import scipy\n",
    "import scipy.stats as stati\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "try:\n",
    "    engine = sqlalchemy.create_engine(\"oracle+cx_oracle://rhdata:rhdata@localhost?service_name=RH\",\n",
    "                                      arraysize=1000)\n",
    "    # retrive gpAffectation from GP_AFFECTATION table\n",
    "    # affectation_sql = \"\"\"select NUM_IM,CODEBUR,LIEU_AF,MOTIF_AFFECT,FONCTION_AF,DIVISION_AF,SERVICE_AF,DIRECTION_AF,DATE_PS from GP_AFFECTATION where date_ps IS NOT NULL\"\"\"\n",
    "    affectation_sql = \"\"\"select NUM_IM,CODEBUR,LIEU_AF,MOTIF_AFFECT,FONCTION_AF,DIVISION_AF,SERVICE_AF,DIRECTION_AF,DATE_PS from GP_AFFECTATION\"\"\"\n",
    "\n",
    "    datah = pd.read_sql(affectation_sql, engine)\n",
    "    # datah = pd.read_csv('./RH.csv', sep=';')\n",
    "    # Collecte de données =>Formatage : nomena colonne reetr, atambatra ze mtov(variable quantitative) -mampiasa var.unique\n",
    "    dfo = pd.DataFrame(datah,  index=datah.index, columns=datah.columns)\n",
    "    print(\"dfo: \",dfo)\n",
    "    print(\"dfo index: \",dfo.index)\n",
    "    print(\"dfo columns: \",dfo.columns)\n",
    "\n",
    "    # détermine si valeur en paramètre est manquante:\n",
    "    def num_missing(x):\n",
    "        return sum(x.isnull())\n",
    "    dfo.apply(num_missing,axis=0)\n",
    "    dfo.apply(num_missing,axis=1).head()\n",
    "    # les individus ou lignes\n",
    "    dfo = dfo.dropna(axis=0)\n",
    "    # les variables ou colonnes\n",
    "    dfo = dfo.dropna(axis=1)\n",
    "    # Supprimer toutes les observations contenant une valeur manquante (au moins sur une variable)\n",
    "    dfo.dropna(how=\"all\", inplace=True) # lorsqu'on veut garder la même table\n",
    "    dfo['service_af'].dropna(how=\"all\", inplace=True) # lorsqu'on veut garder la même table\n",
    "    dfo['Annee'] = dfo['date_ps'].apply(lambda d: d.year)\n",
    "    Annee = dfo['Annee'].fillna(0).copy()\n",
    "    dfo.replace(np.nan,0)\n",
    "    dfo.head()\n",
    "    dfo.info()\n",
    "\n",
    "    # --------------------------- Analyse univarié -----------------------------\n",
    "    # Etude univarié qualitative\n",
    "    # -------- Fréquence Lieu\n",
    "    xlieu = pd.value_counts(dfo['lieu_af'])\n",
    "    dico=dict(xlieu) # Transformer x en un dictionnaire\n",
    "    # Transformation du dictionnaire en dataframe\n",
    "    col = dico.keys() # récupérer les clés du dictionnaires (Ces clés représentent les modalités analysées. Elles seront les variables dans le dataframe à créer).\n",
    "    df = pd.DataFrame(dico,columns=col, index=[0]) # index=[0] doit être spécifié lorsque les valeurs du dictionnaire ne sont pas encadré par [].\n",
    "    df['total']=df.sum(axis=1)\n",
    "    for names, values in df.iteritems(): # on récupère le nom de la colonne et sa valeur\n",
    "        df['{name}'.format(name=names)]=100*df['{name}'.format(name=names)]/df['total'] # Calcul du pourcentage\n",
    "        # print(\"total :\",df['total'])\n",
    "        # print(\"name :\",df['{name}'.format(name=names)])\n",
    "    # print(df) # on obtient ainsi le tableau de fréquence en pourcentage\n",
    "    ax = df.plot(kind='barh',figsize =(10, 8))\n",
    "    plt.title(\"Fréquence de lieu d'affectation\")\n",
    "    plt.show()\n",
    "\n",
    "    # Etude univarié qualitative\n",
    "    # -------- Fréquence Service\n",
    "    xservice = pd.value_counts(dfo['service_af'])\n",
    "    fig, ax = plt.subplots()\n",
    "    labels = xservice.index\n",
    "    labels\n",
    "    ax.pie(xservice, labels=labels, startangle=90)\n",
    "    ax.axis('equal')\n",
    "    plt.title(\"Fréquence de service d'affectation\")\n",
    "    plt.show()\n",
    "\n",
    "      #Regardons maintenant Service tendance en calculant le pourcentage ou le taux. (Mode)\n",
    "    x = dfo['service_af'].mode().value_counts()\n",
    "    dicos=dict(x) # Transformer x en un dictionnaire\n",
    "    print(\"dicos:\",dicos)\n",
    "    # Transformation du dictionnaire en dataframe\n",
    "    col = dicos.keys() # récupérer les clés du dictionnaires (Ces clés représentent les modalités analysées. Elles seront les variables dans le dataframe à créer).\n",
    "    dfT = pd.DataFrame(dicos,columns=col, index=[0]) # index=[0] doit être spécifié lorsque les valeurs du dictionnaire ne sont pas encadré par [].\n",
    "    dfT['total']=dfT.sum(axis=1)\n",
    "    for names, values in dfT.iteritems(): # on récupère le nom de la colonne et sa valeur\n",
    "        dfT['{name}'.format(name=names)]=100*dfT['{name}'.format(name=names)]/dfT['total'] # Calcul du pourcentage\n",
    "        # print(\"total :\",df['total'])\n",
    "        # print(\"name :\",df['{name}'.format(name=names)])\n",
    "    # print(df) # on obtient ainsi le tableau de fréquence en pourcentage\n",
    "    ax = dfT.plot(kind='barh')\n",
    "    plt.title(\"Tendance de service\")\n",
    "    plt.show()\n",
    "\n",
    "     #Regardons maintenant Lieu tendance en calculant le pourcentage ou le taux. (Mode)\n",
    "    xlieu = dfo['lieu_af'].mode().value_counts()\n",
    "    dico=dict(xlieu)\n",
    "    print(\"dico:\",dico)# Transformer x en un dictionnaire\n",
    "    # Transformation du dictionnaire en dataframe\n",
    "    col = dico.keys() # récupérer les clés du dictionnaires (Ces clés représentent les modalités analysées. Elles seront les variables dans le dataframe à créer).\n",
    "    dfTl = pd.DataFrame(dico,columns=col, index=[0]) # index=[0] doit être spécifié lorsque les valeurs du dictionnaire ne sont pas encadré par [].\n",
    "    dfTl['total']=dfT.sum(axis=1)\n",
    "    for names, values in dfTl.iteritems(): # on récupère le nom de la colonne et sa valeur\n",
    "        dfTl['{nameLieu}'.format(nameLieu=names)]=100*dfTl['{nameLieu}'.format(nameLieu=names)]/dfTl['total'] # Calcul du pourcentage\n",
    "        # print(\"total :\",dfTl['total'])\n",
    "        # print(\"nameLieu :\",dfTl['{nameLieu}'.format(nameLieu=names)])\n",
    "    # print(dfTl) # on obtient ainsi le tableau de fréquence en pourcentage\n",
    "    ax = dfTl.plot(kind='barh')\n",
    "    plt.title(\"Tendance de lieu\")\n",
    "    plt.show()\n",
    "\n",
    "    # -------- Proportion de modalité pour chaque variable (service)\n",
    "    g = dfo.groupby('service_af')\n",
    "    print('Il y a', np.shape(g)[0], 'sous-groupes distincts')\n",
    "    plt.hist(dfo['service_af'])\n",
    "    plt.title('Histogramme des Services')\n",
    "    plt.xlabel('types de service')\n",
    "    plt.ylabel('effectifs')\n",
    "    plt.show()\n",
    "\n",
    "    lieu = dfo.groupby('lieu_af')\n",
    "    print('Il y a', np.shape(lieu)[0], 'sous-groupes distincts')\n",
    "    plt.hist(dfo['lieu_af'], color='green')\n",
    "    plt.title('Histogramme des Lieux')\n",
    "    plt.xlabel('types de lieu')\n",
    "    plt.ylabel('effectifs')\n",
    "    plt.show()\n",
    "\n",
    "    # Etude univarié quantitative\n",
    "    # Résumés numériques : moyenne empirique, variance et écart-type, min, max, quantiles, ...\n",
    "    # Graphiques : Histogrammes, boite à moustache, ...\n",
    "    xdate= pd.value_counts(dfo['Annee'])\n",
    "    stat=[dfo['Annee'].min(),dfo['Annee'].max(),dfo['Annee'].mean(),(dfo['Annee'].std())**2,dfo['Annee'].median(),dfo['Annee'].skew(),dfo['Annee'].kurt()]\n",
    "    print(stat) # attention: legère différence dans certaines valeurs kurt et skew.\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    axdate = fig.add_subplot(111)\n",
    "    bp = axdate.boxplot(stat)\n",
    "    bp.values\n",
    "    plt.title(\"Etude univarié\")\n",
    "    axdate.legend(stat)\n",
    "    # for names, values in stat.iteritems(): # on récupère le nom de la colonne et sa valeur\n",
    "    #     stat['{nameSt}'.format(nameSt=names)]=100*stat['{nameSt}'.format(nameSt=names)]/stat['total'] # Calcul du pourcentage\n",
    "        # print(\"total :\",dfTl['total'])\n",
    "        # print(\"nameLieu :\",dfTl['{nameLieu}'.format(nameLieu=names)])\n",
    "    # print(dfTl) # on obtient ainsi le tableau de fréquence en pourcentage\n",
    "    # axdate.legend(\"min\", \"max\",\"mean\",\"std\",\"median\",\"skew\",\"kurt\" [Annee], loc='upper right')\n",
    "    axdate.set_xlim(0,6)\n",
    "    plt.show(bp)\n",
    "\n",
    "    # --------------------------- Analyse bivarié -----------------------------\n",
    "    # val numerique de mjer matrice de corrélation\n",
    "    # val categorielle de mjer independance @ test de khi2\n",
    "\n",
    "    # tri croisé sur colonne service_af et lieu_af afin d'obtenir un tableau de contingence(fréquences absolues)\n",
    "    # Pourcentage par rapport au total\n",
    "    xtotal = pd.crosstab(dfo.service_af, dfo.lieu_af).fillna(0).copy().apply(lambda r: r/len(dfo),axis=0)\n",
    "    sb.set_style(\"whitegrid\")\n",
    "    ct = pd.crosstab(dfo.service_af, dfo.lieu_af).fillna(0).copy()\n",
    "    ct.plot.bar(stacked=True, figsize=(18,15))\n",
    "    plt.legend(title='Fréquence service selon lieu affectation')\n",
    "    plt.show()\n",
    "    st_chi2, st_p, st_dof, st_exp = stati.chi2_contingency(ct)\n",
    "    print(\"chi_2: \",st_chi2)\n",
    "    print(\"p_value: \",st_p)\n",
    "    print(\"degre liberte: \",st_dof)\n",
    "    # print(\"tab freq: \",st_exp)\n",
    "\n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    axcont = fig.add_subplot(111)\n",
    "    axcont.set_aspect(1)\n",
    "    res = sb.heatmap(st_exp, annot=True, fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.title('Tableau de fréquence',fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "     # tri croisé sur colonne motif et service afin d'obtenir un tableau de contingence(fréquences absolues)\n",
    "    # Pourcentage par rapport au total\n",
    "    xtotalservicemotif = pd.crosstab(dfo.service_af, dfo.motif_affect).fillna(0).copy().apply(lambda r: r/len(dfo),axis=0)\n",
    "    sb.set_style(\"whitegrid\")\n",
    "    ctmotif = pd.crosstab(dfo.service_af, dfo.motif_affect).fillna(0).copy()\n",
    "    ctmotif.plot.bar(stacked=True, figsize=(18,15))\n",
    "    plt.legend(title='Fréquence service selon motif affectation')\n",
    "    plt.show()\n",
    "    st_chi2, st_p, st_dof, st_exp = stati.chi2_contingency(ctmotif)\n",
    "    print(\"chi_2: \",st_chi2)\n",
    "    print(\"p_value: \",st_p)\n",
    "    print(\"degre liberte: \",st_dof)\n",
    "    # print(\"tab freq: \",st_exp)\n",
    "\n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    axservmotif = fig.add_subplot(111)\n",
    "    axservmotif.set_aspect(1)\n",
    "    reservmotif = sb.heatmap(st_exp, annot=True, fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.title('Tableau de fréquence',fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # tri croisé sur colonne service_af et date_ps afin d'obtenir un tableau de contingence(fréquences absolues)\n",
    "    serviceDate = pd.crosstab(dfo.service_af, Annee).fillna(0).copy()\n",
    "    print(\"serviceDate:\",serviceDate)\n",
    "    # Fréquences relatives # Pourcentage par rapport au total\n",
    "    xtotalSd = serviceDate.apply(lambda r: r/len(df),axis=0)\n",
    "    sb.set_style(\"whitegrid\")\n",
    "    sb.boxplot(x = dfo['service_af'], y = 'Annee', data = dfo)\n",
    "    plt.title(\"Fréquence de service par date prise de service\")\n",
    "    plt.show()\n",
    "    st_chi2, st_p, st_dof, st_exp = stati.chi2_contingency(serviceDate)\n",
    "    print(\"chi_2: \",st_chi2)\n",
    "    print(\"p_value: \",st_p)\n",
    "    print(\"degre liberte: \",st_dof)\n",
    "    # print(\"tab freq: \",st_exp)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    correlation = serviceDate.corr()\n",
    "    sb.heatmap(correlation,vmax=1,vmin=-1,square=True,annot=True,linewidths=.5,cmap=\"YlGnBu\")\n",
    "    plt.show()\n",
    "\n",
    "    # Test de normalité (si distribution features suit loi normal) NB : relation variable avec label\n",
    "    # identification loi normal ou test non parametrique\n",
    "    def shapiro_test(x):\n",
    "        try :\n",
    "            resh = stat.shapiro(x)\n",
    "        except :\n",
    "            return -1\n",
    "        alpha = 0.05\n",
    "        print(\"p = \",resh.pvalue)\n",
    "        if resh.pvalue < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "\n",
    "            print(\"(shapiro)The null hypothesis can be rejected -> X ne possède pas une distribution normale\")\n",
    "\n",
    "        else:\n",
    "            print(\"(shapiro) The null hypothesis cannot be rejected -> X possède éventuellement une distribution normale\")\n",
    "        return resh\n",
    "\n",
    "\n",
    "    def omnibus_normaltest(x):\n",
    "        #test D'Agostino-Pearson\n",
    "        try :\n",
    "            k2, p = stat.normaltest(x)\n",
    "        except :\n",
    "            return -1\n",
    "\n",
    "        alpha = 0.05\n",
    "\n",
    "        print(\"p = \",p)\n",
    "\n",
    "        if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "\n",
    "            print(\"(normaltest) The null hypothesis can be rejected -> X ne possède pas une distribution normale\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(\"(normaltest) The null hypothesis cannot be rejected -> X possède éventuellement une distribution normale\")\n",
    "\n",
    "        return [k2,p]\n",
    "\n",
    "    # -------------------------------- Préparation des données -----------------------------\n",
    "    datah.info()\n",
    "    n = 70\n",
    "    nbhead = int(len(datah)*(n/100))\n",
    "    gpAffectation = datah.head(nbhead)\n",
    "    dfo = pd.DataFrame(gpAffectation,  index=gpAffectation.index, columns=gpAffectation.columns)\n",
    "    # détermine si valeur en paramètre est manquante:\n",
    "    def num_missing(x):\n",
    "        return sum(x.isnull())\n",
    "    dfo.apply(num_missing,axis=0)\n",
    "    dfo.apply(num_missing,axis=1).head()\n",
    "    # les individus ou lignes\n",
    "    dfo = dfo.dropna(axis=0)\n",
    "    # les variables ou colonnes\n",
    "    dfo = dfo.dropna(axis=1)\n",
    "    # Supprimer toutes les observations contenant une valeur manquante (au moins sur une variable)\n",
    "    dfo.dropna(how=\"all\", inplace=True) # lorsqu'on veut garder la même table\n",
    "    dfo['service_af'].dropna(how=\"all\", inplace=True) # lorsqu'on veut garder la même table\n",
    "    dfo.replace(np.nan,0)\n",
    "    dfo['Annee'] = dfo['date_ps'].apply(lambda d: d.year)\n",
    "    Annee = dfo['Annee'].fillna(0).copy().astype(int)\n",
    "    dfo.head()\n",
    "    dfo.info()\n",
    "\n",
    "\n",
    "    # effectifs = dfo[\"service_af\"].value_counts()\n",
    "    # modalites = effectifs.index # l'index de effectifs contient les modalités\n",
    "    #\n",
    "    # tab = pd.DataFrame(modalites, columns = [\"service_af\"]) # création du tableau à partir des modalités\n",
    "    # tab[\"n\"] = effectifs.values\n",
    "    # tab[\"f\"] = tab[\"n\"] / len(dfo) # len(data) renvoie la taille de l'échantillon\n",
    "    # tab = tab.sort_values(\"service_af\") # tri des valeurs de la variable X (croissant)\n",
    "    # print(\"tableau:\", tab)\n",
    "\n",
    "\n",
    "\n",
    "    #Examinons maintenant l'evolution des affectations\n",
    "    sb.relplot(Annee,\"num_im\",\"service_af\",data=dfo,height=6,s=70)\n",
    "    plt.title('Evolution de l affectation')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sb.countplot(x=Annee,data=dfo, palette='rainbow',hue='service_af')\n",
    "    plt.xlabel(\"Affectation\")\n",
    "    plt.ylabel(\"Effectif\")\n",
    "    plt.title(\"Effectif des agents par Service\")\n",
    "\n",
    "    # Change this scatter plot to arrange the plots in rows instead of columns\n",
    "    # sb.relplot(x= Annee, y= 'service_af', data=dfo, kind=\"scatter\",row=Annee)\n",
    "\n",
    "\n",
    "\n",
    "    # Mesure d’association entre variables qualitatives : le test d'indépendance de khi-deux (stat khi-deux, pvalue, ddl, et tableau de fréquence théorique)\n",
    "    X = dfo['Annee']\n",
    "    Y = dfo['service_af']\n",
    "    cont = dfo[['Annee', 'service_af']].pivot_table(index=X, columns=Y, aggfunc=len).fillna(0).copy()\n",
    "    tx = X.value_counts()\n",
    "    ty = Y.value_counts()\n",
    "    cont = cont.astype(int)\n",
    "    st_chi2, st_p, st_dof, st_exp = stati.chi2_contingency(cont)\n",
    "    print(\"chi_2: \",st_chi2)\n",
    "    print(\"p_value: \",st_p)\n",
    "    print(\"degre liberte: \",st_dof)\n",
    "    print(\"tab freq: \",st_exp)\n",
    "\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}